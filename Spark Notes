Starting:
Move to spark cluster column and run:
docker-compose up --scale spark-worker=3

SPARK Structured API: DataFrames
---------------------------------

DataFrame is a distributed collection of rows conforming to a schema.
schema -> list describing the column names and types
DF is a distributed spreadsheet with rows and columns.
The datatypes are known to spark when the dataframe is being used, not at compile time.

The data needs to be distributed, because 
- data too big for a single computer
- too long to process the entire data on a single computer

So, it needs Partitioning
- splits the data into files, distributed between nodes in the cluster
- impacts processing parallelism of the data

Dataframes
- immutable 
    - cannot be changed once created
    - if you want to modify them, then create new DFs via transformations

- Transformations
    - narrow transformation : one input partition contributes to at most one output partition (e.g map)
    - wide transformation : one or more input partition create many output partitions (e.g. sort)

Shuffle -> Data exchange between cluster nodes
- occurs in wide Transformations
- affects performance massively

Lazy evaluation
- Spark waits untill the last moment to execute the DF transformations

Planning
-Spark compiles the DF transfomrations into a graph before running any code
- Logical plan = DF dependency graph + all the narrow and wide transfomrations that spark will have to execute. So spark will know every single step in advance.
- Physical plan = spark will optimize a sequence of steps and it will know which node will execute which part of the transfomration
- Optimization - Since spark knows all the steps in advance so it will include optimization in its plans.

Transformations vs Actions
- Transformations describe how new DFs are obtained. Actions(like show(), count()) actually start executing the code.

Example:
> .option("mode", "failFast")
Throws an exception when it gets datatype mismatch.
org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST.
To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.

Parquet:
An Open source compressed binary data storage format, optized for fast reading of columns. It is the default storage format for DFs as it works very well with Spark.
It is very predictable, so we donot need so many options as we need in json or csv.

NB. Scala provides a helper class, called App, that provides the main method. 
Instead of writing your own main method, classes can extend the App class to produce concise and executable applications in Scala.
App is a trait which is utilized to rapidly change objects into feasible programs, which is carried out by applying DelayedInit function and the objects 
inheriting the trait App uses this function to execute the entire body of the program as a section of an inherited main method.

# Columns and expressions
DataFrames are actually split in partitions in between nodes in the clusters. So when we select 2 columns or any number of columns from a data frame, 
those columns get selected on every partition on every node where the data frame resides.
After select, we will obtain a new data frame with those 2 columns that we wanted and that will be reflected on every node in the cluster.
This is called narrow transformation. 
That means every input partition in the original dataframe has exactly one corresponding output partition in the resulting dataframe.

#EXPRESSIONS are a powerful construct and will alow us to process data frames in almost any fashion we like.

Wide Transformations:
One or more input partitions contribute to one or more output partitions.
Shuffles: Data is being moved in between different nodes in a Spark cluster. Shuffling is an expensive operation.
If you have data pipelines that you want to be really fast, it is best to do data aggregations and grouping at the end of the processing.
Joins are wide transfomrations (i.e expensive).

#Managing nulls:

Managing the absence of data.
Non-nullable column doesnot mean that the column will not have null values ever.
- It is not a constraint
- These are marker for Spark to optimize for nulls
- Can lead to exceptions or data errors, if broken

Datasets:
Typed dataframes : distributed collection of jvm objects
Most useful when:
- we want to maintain type Information
- we want clean concise code
- our filters/transfomrations are hard to express in DF or SQL
Avoid when:
- performance is critical: Spark cannot optimize transfomrations
N.B. All dataset has access to all DF functions

#RDD : Resilient Distributed Datasets




SPARK SQL
----------

SQL
- Universal language for accessing structured data
- Acts as an abstraction over DFs for engineers familiar with databases
- Supported in Spark
    - programmatically in expressions
    - in the Spark shell
- Spark SQL
    - has the concept of database, table, view
    - allows accessing DFs as tables

DataFrame vs table
- Identical in terms of storage and Partitioning
- DFs can be processed programmatically, tables in SQL

> docker exec -it sparkcluster_spark-master_1 bash
Will automtically connect to the docker container containing the master node and we execute a bash shell

Querying with SPARK SQL:

root@e92172a15dd6:/# ls
bin  boot  dev	etc  home  lib	lib64  media  mnt  opt	proc  root  run  sbin  spark  srv  start-master.sh  sys  tmp  usr  var
root@e92172a15dd6:/# cd spark/
root@e92172a15dd6:/spark# ./bin/spark-sql
- This will run a sql shell on this docker container. This will start a sql shell for us to operate on. A prompt will spark-sql will appear.

spark-sql> show databases;
default
Time taken: 2.396 seconds, Fetched 1 row(s)

spark-sql> create database rtjvm;
spark-sql> use rtjvm;

- Ctrl+L to clear the sql prompt

spark-sql> insert into persons values (1, "Amy Adams"), (2, "Bablu Bechara"), (3, "Chandan Pal"), (4, "Daniel Day Lewis"), (5, "Evan Rajoski");
spark-sql> select * from persons;
1	Amy Adams
3	Chandan Pal
5	Evan Rajoski
2	Bablu Bechara
4	Daniel Day Lewis
Time taken: 0.283 seconds, Fetched 5 row(s)

> MANAGED table means spark is responsible for the metadata of the table and the data.
If we drop the table Persons then we will lose the data.

Check the data storage in managed table like this:-
cc@cc-HP-Laptop-15-da0xxx:/media/cc/Data1/github/spark-essentials$ docker exec -it sparkcluster_spark-master_1 bash
root@e92172a15dd6:/# ls
bin  boot  dev	etc  home  lib	lib64  media  mnt  opt	proc  root  run  sbin  spark  srv  start-master.sh  sys  tmp  usr  var
root@e92172a15dd6:/# cd spark/
root@e92172a15dd6:/spark# ls
LICENSE  NOTICE  R  README.md  RELEASE	bin  conf  data  derby.log  examples  jars  kubernetes	licenses  logs	metastore_db  python  sbin  spark-warehouse  yarn
root@e92172a15dd6:/spark# cd spark-warehouse
root@e92172a15dd6:/spark/spark-warehouse# ls
rtjvm.db
root@e92172a15dd6:/spark/spark-warehouse# cd rtjvm.db/
root@e92172a15dd6:/spark/spark-warehouse/rtjvm.db# ls
persons
root@e92172a15dd6:/spark/spark-warehouse/rtjvm.db# cd persons/
root@e92172a15dd6:/spark/spark-warehouse/rtjvm.db/persons# ls -la
total 48
drwxr-xr-x 2 root 1000 4096 Apr  4 10:35 .
drwxr-xr-x 3 root 1000 4096 Apr  4 10:31 ..
-rwxr-xr-x 1 root 1000   12 Apr  4 10:35 .part-00000-f5fddc84-3ec1-4318-ab3b-b31b69882103-c000.crc
-rwxr-xr-x 1 root 1000   12 Apr  4 10:35 .part-00001-f5fddc84-3ec1-4318-ab3b-b31b69882103-c000.crc
-rwxr-xr-x 1 root 1000   12 Apr  4 10:35 .part-00002-f5fddc84-3ec1-4318-ab3b-b31b69882103-c000.crc
-rwxr-xr-x 1 root 1000   12 Apr  4 10:35 .part-00003-f5fddc84-3ec1-4318-ab3b-b31b69882103-c000.crc
-rwxr-xr-x 1 root 1000   12 Apr  4 10:35 .part-00004-f5fddc84-3ec1-4318-ab3b-b31b69882103-c000.crc
-rwxr-xr-x 1 root 1000   12 Apr  4 10:35 part-00000-f5fddc84-3ec1-4318-ab3b-b31b69882103-c000
-rwxr-xr-x 1 root 1000   16 Apr  4 10:35 part-00001-f5fddc84-3ec1-4318-ab3b-b31b69882103-c000
-rwxr-xr-x 1 root 1000   14 Apr  4 10:35 part-00002-f5fddc84-3ec1-4318-ab3b-b31b69882103-c000
-rwxr-xr-x 1 root 1000   19 Apr  4 10:35 part-00003-f5fddc84-3ec1-4318-ab3b-b31b69882103-c000
-rwxr-xr-x 1 root 1000   15 Apr  4 10:35 part-00004-f5fddc84-3ec1-4318-ab3b-b31b69882103-c000

The five files (ending with c000) are the 5 partitions created in the table.

> External table:-
- Spark is in charge of the metadata only
- Even if we drop the table, we keep  the data as the actual data is in some other place. This is very useful when we have some hive or S3 files that
we want to keep for ourselves so that they are not managed by spark. So if we drop the table we can keep the data and the data is our responsibility.

//Creating an external table
create table flights(origin string, destination string) using csv options(header true, path '/home/rtjvm/data/flights')

//Inserting data
insert into flights values ("Kolkata", "Chennai"), ("Kolkata", "Bangalore"), ("Bangalore", "Delhi"), ("Delhi", "Guwahati"), ("Bhopal", "Kochi");
select * from flights;

//Checking data
cd /home/rtjvm/data/flights
root@e92172a15dd6:/home/rtjvm/data/flights# ls -l
total 20
-rw-r--r-- 1 root root  0 Apr  4 12:13 _SUCCESS
-rw-r--r-- 1 root root 35 Apr  4 12:13 part-00000-0fb5ea1e-1085-4ca2-8760-51bd9dd9b0be-c000.csv
-rw-r--r-- 1 root root 37 Apr  4 12:13 part-00001-0fb5ea1e-1085-4ca2-8760-51bd9dd9b0be-c000.csv
-rw-r--r-- 1 root root 35 Apr  4 12:13 part-00002-0fb5ea1e-1085-4ca2-8760-51bd9dd9b0be-c000.csv
-rw-r--r-- 1 root root 34 Apr  4 12:13 part-00003-0fb5ea1e-1085-4ca2-8760-51bd9dd9b0be-c000.csv
-rw-r--r-- 1 root root 32 Apr  4 12:13 part-00004-0fb5ea1e-1085-4ca2-8760-51bd9dd9b0be-c000.csv

spark-sql> describe extened flights;
20/04/04 12:27:15 INFO HiveMetaStore: 0: get_table : db=rtjvm tbl=extened
20/04/04 12:27:15 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=rtjvm tbl=extened	
Error in query: Table or view not found: extened;
spark-sql> describe extended flights;
20/04/04 12:27:25 INFO HiveMetaStore: 0: get_database: rtjvm
20/04/04 12:27:25 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: rtjvm	
20/04/04 12:27:25 INFO HiveMetaStore: 0: get_table : db=rtjvm tbl=flights
20/04/04 12:27:25 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=rtjvm tbl=flights	
20/04/04 12:27:25 INFO HiveMetaStore: 0: get_table : db=rtjvm tbl=flights
20/04/04 12:27:25 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=rtjvm tbl=flights	
origin	string	NULL
destination	string	NULL
		
# Detailed Table Information		
Database	rtjvm	
Table	flights	
Owner	root	
Created Time	Sat Apr 04 12:09:17 UTC 2020	
Last Access	Thu Jan 01 00:00:00 UTC 1970	
Created By	Spark 2.4.3	
Type	EXTERNAL	
Provider	csv	
Table Properties	[transient_lastDdlTime=1586002157]	
Location	file:/home/rtjvm/data/flights	
Serde Library	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	
InputFormat	org.apache.hadoop.mapred.SequenceFileInputFormat	
OutputFormat	org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat	
Storage Properties	[header=true, serialization.format=1]	
Time taken: 0.074 seconds, Fetched 18 row(s)
20/04/04 12:27:25 INFO SparkSQLCLIDriver: Time taken: 0.074 seconds, Fetched 18 row(s)

The 5 files are created for the 5 rows inserted in the table. 
The table type is mentioned as EXTERNAL.